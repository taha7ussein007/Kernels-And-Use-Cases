{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VXPfRp-tARR",
    "outputId": "6c27acaf-2c6a-467f-fd7a-68efce98e30b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import csv\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZihvUPvHtARd"
   },
   "source": [
    "<h1>4. Machine Learning Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtN9VBPutARf"
   },
   "source": [
    "<h2> 4.1 Reading data from file and storing into sql table </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owBQdjY1tARh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 rows\n",
      "360000 rows\n",
      "540000 rows\n"
     ]
    }
   ],
   "source": [
    "#Creating db file from csv\n",
    "if not os.path.isfile('../Datasets/Quora_Questions/train.db'):\n",
    "    disk_engine = create_engine('sqlite:///../Datasets/Quora_Questions/train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('../Datasets/Quora_Questions/final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hpD3aBktARn"
   },
   "outputs": [],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "\n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR8ZIUnttARs",
    "outputId": "810fb3fb-7da2-4b78-9e29-9edabbf68cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "read_db = '../Datasets/Quora_Questions/train.db'\n",
    "conn_r = create_connection(read_db)\n",
    "checkTableExists(conn_r)\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZq5gaaztARy"
   },
   "outputs": [],
   "source": [
    "# try to sample data according to the computing power you have\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        # for selecting first 1M rows\n",
    "        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n",
    "        \n",
    "        # for selecting random points\n",
    "        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n",
    "        conn_r.commit()\n",
    "        conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkeBKktKtAR3"
   },
   "outputs": [],
   "source": [
    "# remove the first row \n",
    "data.drop(data.index[0], inplace=True)\n",
    "y_true = data['is_duplicate']\n",
    "data.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKSenpsmtAR9",
    "outputId": "81d890ce-df79-4402-9324-84817dbd5a7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.749981250468738</td>\n",
       "      <td>0.599988000239995</td>\n",
       "      <td>0.999950002499875</td>\n",
       "      <td>0.499987500312492</td>\n",
       "      <td>0.833319444675922</td>\n",
       "      <td>0.555549382784636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.23644759878516</td>\n",
       "      <td>5.30523228645325</td>\n",
       "      <td>3.13555148243904</td>\n",
       "      <td>-4.75458674132824</td>\n",
       "      <td>10.1221147179604</td>\n",
       "      <td>5.60770386457443</td>\n",
       "      <td>-13.2198683372699</td>\n",
       "      <td>14.7027367651463</td>\n",
       "      <td>14.0659053623676</td>\n",
       "      <td>-5.04125234484673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.999966667777741</td>\n",
       "      <td>0.999966667777741</td>\n",
       "      <td>0.749981250468738</td>\n",
       "      <td>0.749981250468738</td>\n",
       "      <td>0.857130612419823</td>\n",
       "      <td>0.857130612419823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.36388489604</td>\n",
       "      <td>9.68169342726469</td>\n",
       "      <td>2.84655494242907</td>\n",
       "      <td>-0.812586778658442</td>\n",
       "      <td>11.1012060642242</td>\n",
       "      <td>1.45815153978765</td>\n",
       "      <td>-9.10422356426716</td>\n",
       "      <td>-7.32554787397385</td>\n",
       "      <td>6.66168880462647</td>\n",
       "      <td>1.44938331842423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.799984000319994</td>\n",
       "      <td>0.571420408279882</td>\n",
       "      <td>0.999950002499875</td>\n",
       "      <td>0.285710204139941</td>\n",
       "      <td>0.857130612419823</td>\n",
       "      <td>0.428568367368804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.52808537706733</td>\n",
       "      <td>23.2864231616259</td>\n",
       "      <td>11.1162437796593</td>\n",
       "      <td>-13.0648375749588</td>\n",
       "      <td>5.5535314232111</td>\n",
       "      <td>14.8614038787782</td>\n",
       "      <td>-21.0363407600671</td>\n",
       "      <td>8.20216829329729</td>\n",
       "      <td>1.03531628288329</td>\n",
       "      <td>-7.99897107481957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.19999800002</td>\n",
       "      <td>0.166665277789352</td>\n",
       "      <td>0.624992187597655</td>\n",
       "      <td>0.416663194473379</td>\n",
       "      <td>0.368419113583613</td>\n",
       "      <td>0.241378478005248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.6301284059882</td>\n",
       "      <td>6.88525542151183</td>\n",
       "      <td>10.9249808937311</td>\n",
       "      <td>-25.4072197750211</td>\n",
       "      <td>41.0658232495189</td>\n",
       "      <td>27.8964016512036</td>\n",
       "      <td>-6.58016937971115</td>\n",
       "      <td>32.1070380061865</td>\n",
       "      <td>34.0372865945101</td>\n",
       "      <td>11.0198228359222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.33332222259258</td>\n",
       "      <td>0.33332222259258</td>\n",
       "      <td>0.499987500312492</td>\n",
       "      <td>0.399992000159997</td>\n",
       "      <td>0.428565306209911</td>\n",
       "      <td>0.374995312558593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>15.1992308162153</td>\n",
       "      <td>9.28573462367058</td>\n",
       "      <td>6.86820656061173</td>\n",
       "      <td>-10.9045005142689</td>\n",
       "      <td>2.939671613276</td>\n",
       "      <td>-5.90711626410484</td>\n",
       "      <td>-13.9218243807554</td>\n",
       "      <td>3.05322687327862</td>\n",
       "      <td>4.86292281746864</td>\n",
       "      <td>-3.01618479192257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cwc_min            cwc_max            csc_min            csc_max  \\\n",
       "1  0.749981250468738  0.599988000239995  0.999950002499875  0.499987500312492   \n",
       "2  0.999966667777741  0.999966667777741  0.749981250468738  0.749981250468738   \n",
       "3  0.799984000319994  0.571420408279882  0.999950002499875  0.285710204139941   \n",
       "4      0.19999800002  0.166665277789352  0.624992187597655  0.416663194473379   \n",
       "5   0.33332222259258   0.33332222259258  0.499987500312492  0.399992000159997   \n",
       "\n",
       "             ctc_min            ctc_max last_word_eq first_word_eq  \\\n",
       "1  0.833319444675922  0.555549382784636          1.0           0.0   \n",
       "2  0.857130612419823  0.857130612419823          1.0           1.0   \n",
       "3  0.857130612419823  0.428568367368804          0.0           1.0   \n",
       "4  0.368419113583613  0.241378478005248          0.0           0.0   \n",
       "5  0.428565306209911  0.374995312558593          0.0           1.0   \n",
       "\n",
       "  abs_len_diff mean_len  ...              374_y             375_y  \\\n",
       "1          3.0      7.5  ...  -6.23644759878516  5.30523228645325   \n",
       "2          0.0      7.0  ...     10.36388489604  9.68169342726469   \n",
       "3          7.0     10.5  ...   2.52808537706733  23.2864231616259   \n",
       "4         10.0     24.0  ...   44.6301284059882  6.88525542151183   \n",
       "5          1.0      7.5  ...   15.1992308162153  9.28573462367058   \n",
       "\n",
       "              376_y               377_y             378_y              379_y  \\\n",
       "1  3.13555148243904   -4.75458674132824  10.1221147179604   5.60770386457443   \n",
       "2  2.84655494242907  -0.812586778658442  11.1012060642242   1.45815153978765   \n",
       "3  11.1162437796593   -13.0648375749588   5.5535314232111   14.8614038787782   \n",
       "4  10.9249808937311   -25.4072197750211  41.0658232495189   27.8964016512036   \n",
       "5  6.86820656061173   -10.9045005142689    2.939671613276  -5.90711626410484   \n",
       "\n",
       "               380_y              381_y             382_y              383_y  \n",
       "1  -13.2198683372699   14.7027367651463  14.0659053623676  -5.04125234484673  \n",
       "2  -9.10422356426716  -7.32554787397385  6.66168880462647   1.44938331842423  \n",
       "3  -21.0363407600671   8.20216829329729  1.03531628288329  -7.99897107481957  \n",
       "4  -6.58016937971115   32.1070380061865  34.0372865945101   11.0198228359222  \n",
       "5  -13.9218243807554   3.05322687327862  4.86292281746864  -3.01618479192257  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaWHDzqUtASD"
   },
   "source": [
    "<h2> 4.2 Converting strings to numerics </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we read from sql table each entry was read it as a string\n",
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(data.columns)\n",
    "for i in cols:\n",
    "    data[i] = pd.to_numeric(data[i], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LpfQwc9tASJ"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7368789/convert-all-strings-in-a-list-to-int\n",
    "y_true = list(map(int, y_true.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuMTqWGutASO"
   },
   "source": [
    "<h2> 4.3 Random train test split( 70:30) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Rat2obGtASP"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8ed0d4ea83f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Latest_Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2207\u001b[0m                      random_state=random_state)\n\u001b[0;32m   2208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2209\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n",
      "\u001b[1;32m~\\Latest_Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[0;32m   1314\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Latest_Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m             raise ValueError(\"The least populated class in y has only 1\"\n\u001b[0m\u001b[0;32m   1696\u001b[0m                              \u001b[1;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m                              \u001b[1;34m\" number of groups for any class cannot\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Iw9zCHqtASS",
    "outputId": "910b684b-0876-4dd8-e0d9-457846236833"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\",X_train.shape)\n",
    "print(\"Number of data points in test data :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oDV15LJtASY",
    "outputId": "70a1e4eb-3f31-4f1e-a53b-ad972978505d"
   },
   "outputs": [],
   "source": [
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "train_distr = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n",
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "test_distr = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfxcPT6jtASg"
   },
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices given y_i, y_i_hat.\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    #divid each element of the confusion matrix with the sum of elements in that column\n",
    "    \n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.T = [[1, 3],\n",
    "    #        [2, 4]]\n",
    "    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =1) = [[3, 7]]\n",
    "    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n",
    "    #                           [2/3, 4/7]]\n",
    "\n",
    "    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n",
    "    #                           [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    #divid each element of the confusion matrix with the sum of elements in that row\n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =0) = [[4, 6]]\n",
    "    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n",
    "    #                      [3/4, 4/6]] \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [1,2]\n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UStQJ5F_tASk"
   },
   "source": [
    "<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwMDqcU7tASl",
    "outputId": "c1e90d53-25ec-445b-e33a-299538520e32"
   },
   "outputs": [],
   "source": [
    "# we need to generate 9 numbers and the sum of numbers should be 1\n",
    "# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n",
    "# ref: https://stackoverflow.com/a/18662466/4084039\n",
    "# we create a output array that has exactly same size as the CV data\n",
    "predicted_y = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    rand_probs = np.random.rand(1,2)\n",
    "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n",
    "\n",
    "predicted_y =np.argmax(predicted_y, axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgY29g_qtASq"
   },
   "source": [
    "<h2> 4.4 Logistic Regression with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb2tOE3GtASr",
    "outputId": "d7e4fc88-7d4e-4313-cda7-462a2409292e"
   },
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ouQSEnr3tASy"
   },
   "source": [
    "<h2> 4.5 Linear SVM with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOFfZ5PLtAS0",
    "outputId": "d31eb598-e275-48cb-c49b-98e9eb76d8ba"
   },
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhTJgclztAS6"
   },
   "source": [
    "<h2> 4.6 XGBoost </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U367-xetAS7",
    "outputId": "167e8588-2ac4-4c6d-ac22-f56a2fce5657"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n",
    "\n",
    "xgdmat = xgb.DMatrix(X_train,y_train)\n",
    "predict_y = bst.predict(d_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6U5b17AatAS_",
    "outputId": "ca83b680-023b-4bc5-f499-8d8d85c2ff5e"
   },
   "outputs": [],
   "source": [
    "predicted_y =np.array(predict_y>0.5,dtype=int)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmiIgHOJtATF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWS6JoB0tATF"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.ML_models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
